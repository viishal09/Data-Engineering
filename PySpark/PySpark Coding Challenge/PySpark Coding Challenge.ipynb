{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be63cbb2-e70f-496b-a935-5b1c74b7f371",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join\n+---+-----+---+-----------+\n| id| name|age|       city|\n+---+-----+---+-----------+\n|  1| John| 25|   New York|\n|  2|Alice| 30|Los Angeles|\n|  3|  Bob| 28|    Chicago|\n+---+-----+---+-----------+\n\nGroup By and Aggregations\n+-----+-----------+\n| name|average_age|\n+-----+-----------+\n| John|       25.0|\n|Alice|       30.0|\n|  Bob|       28.0|\n+-----+-----------+\n\nDrop\n+---+-----+\n| id| name|\n+---+-----+\n|  1| John|\n|  2|Alice|\n|  3|  Bob|\n+---+-----+\n\nSort\n+---+-----+---+\n| id| name|age|\n+---+-----+---+\n|  1| John| 25|\n|  3|  Bob| 28|\n|  2|Alice| 30|\n+---+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame Operations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample DataFrames\n",
    "df1 = spark.createDataFrame([(1, 'John', 25),\n",
    "                              (2, 'Alice', 30),\n",
    "                              (3, 'Bob', 28)],\n",
    "                             ['id', 'name', 'age'])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, 'New York'),\n",
    "                              (2, 'Los Angeles'),\n",
    "                              (3, 'Chicago')],\n",
    "                             ['id', 'city'])\n",
    "\n",
    "# Joining DataFrames\n",
    "joined_df = df1.join(df2, on='id')\n",
    "\n",
    "# GroupBy and Aggregations\n",
    "grouped_df = df1.groupBy('name').agg(avg('age').alias('average_age'))\n",
    "\n",
    "# Droping\n",
    "df1_drop = df1.drop('age')\n",
    "\n",
    "# Sorting\n",
    "sorted_df1 = df1.orderBy('age')\n",
    "\n",
    "# Displaying results\n",
    "print(\"Join\")\n",
    "joined_df.show()\n",
    "print(\"Group By and Aggregations\")\n",
    "grouped_df.show()\n",
    "print(\"Drop\")\n",
    "df1_drop.show()\n",
    "print(\"Sort\")\n",
    "sorted_df1.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c927bc-1780-439e-9012-347776270395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Function Result - \n    Name  Age         City  Salary\n0   John   25     New York   50000\n1  Alice   30  Los Angeles   60000\n2    Bob   28      Chicago   55000\nGroup BY and Aggregation Function Result -\n        Age\nName       \nAlice  30.0\nBob    28.0\nJohn   25.0\nDrop Function Result - \n    Name         City\n0   John     New York\n1  Alice  Los Angeles\n2    Bob      Chicago\nSort Function Result - \n    Name  Age         City\n0   John   25     New York\n2    Bob   28      Chicago\n1  Alice   30  Los Angeles\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'Name': ['John', 'Alice', 'Bob'],\n",
    "        'Age': [25, 30, 28],\n",
    "        'City': ['New York', 'Los Angeles', 'Chicago']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Joining DataFrames\n",
    "data2 = {'Name': ['John', 'Alice', 'Bob'],\n",
    "         'Salary': [50000, 60000, 55000]}\n",
    "df2 = pd.DataFrame(data2)\n",
    "merged_df = pd.merge(df, df2, on='Name')\n",
    "\n",
    "# GroupBy and Aggregations\n",
    "grouped_df = df.groupby('Name').agg({'Age': 'mean'})\n",
    "\n",
    "# Dropping\n",
    "df_dropped = df.drop(columns=['Age'])\n",
    "\n",
    "# Sorting\n",
    "sorted_df = df.sort_values(by='Age')\n",
    "\n",
    "# Displaying results\n",
    "print(\"Merged Function Result - \")\n",
    "print(merged_df)\n",
    "print(\"Group BY and Aggregation Function Result -\")\n",
    "print(grouped_df)\n",
    "print(\"Drop Function Result - \")\n",
    "print(df_dropped)\n",
    "print(\"Sort Function Result - \")\n",
    "print(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db438ac4-e4fc-4d08-83e8-026fd4eee9d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----------+\n| id| name|age|       city|\n+---+-----+---+-----------+\n|  1| John| 25|   New York|\n|  2|Alice| 30|Los Angeles|\n|  3|  Bob| 28|    Chicago|\n+---+-----+---+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL Joins\") \\\n",
    "    .getOrCreate()\n",
    "# Create sample DataFrames\n",
    "df1 = spark.createDataFrame([(1, 'John', 25),\n",
    "                             (2, 'Alice', 30),\n",
    "                             (3, 'Bob', 28)],\n",
    "                            ['id', 'name', 'age'])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, 'New York'),\n",
    "                             (2, 'Los Angeles'),\n",
    "                             (3, 'Chicago')],\n",
    "                            ['id', 'city'])\n",
    "# Register DataFrames as temporary views\n",
    "df1.createOrReplaceTempView(\"df1_view\")\n",
    "df2.createOrReplaceTempView(\"df2_view\")\n",
    "# Perform SQL join\n",
    "joined_df = spark.sql(\"SELECT df1_view.*, df2_view.city FROM df1_view JOIN df2_view ON df1_view.id = df2_view.id\")\n",
    "# Displaying results\n",
    "joined_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69834a05-b32f-4f76-85b1-38dd3c8acf9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Name  Age\n0   John_suffix   25\n1  Alice_suffix   30\n2    Bob_suffix   28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "data = {'Name': ['John', 'Alice', 'Bob'],\n",
    "        'Age': [25, 30, 28]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a custom function\n",
    "def add_suffix(name):\n",
    "    return name + \"_suffix\"\n",
    "\n",
    "# Apply function to a column\n",
    "df['Name'] = df['Name'].apply(add_suffix)\n",
    "\n",
    "# Displaying results\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2024-02-12 11:04:56",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
